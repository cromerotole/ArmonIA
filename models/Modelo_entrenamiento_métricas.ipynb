{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMYzOkRJbOFQ/4uMOvCoo/S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## SETUP"],"metadata":{"id":"sbTNa0yqNl5k"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TwerOtVPIDKH","executionInfo":{"status":"ok","timestamp":1755104244139,"user_tz":-120,"elapsed":64479,"user":{"displayName":"Carlos Romero","userId":"09804593201902616528"}},"outputId":"488b4f2d-3d6a-4049-c41c-c28268948941"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Setup: rutas, helpers de datos, dataset/splits/loaders, modelo y métricas (sin entrenar).\n","try:\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","except Exception:\n","    pass\n","\n","# --- Imports y constantes ---\n","import os, json, math, random, ast\n","from dataclasses import dataclass, asdict\n","from typing import List, Tuple, Dict\n","import numpy as np, pandas as pd\n","import torch, torch.nn as nn, torch.nn.functional as F\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","\n","SEED = 42\n","random.seed(SEED); np.random.seed(SEED)\n","torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# --- Rutas del proyecto ---\n","BASE_DIR = \"/content/drive/MyDrive/Colab Notebooks/TFG\"\n","DATA_DIR = os.path.join(BASE_DIR, \"Archivos preprocesamiento\")  # artefactos del notebook 1\n","MODELS_DIR = os.path.join(BASE_DIR, \"models\")                   # mismos nombres/ubicación que el original\n","os.makedirs(MODELS_DIR, exist_ok=True)\n","\n","DATA_PARQUET = os.path.join(DATA_DIR, \"all_features_transformer.parquet\")\n","DATA_CSV     = os.path.join(DATA_DIR, \"all_features_transformer.csv\")\n","CH2IDX_PATH  = os.path.join(DATA_DIR, \"chord_to_idx.json\")\n","IDX2CH_PATH  = os.path.join(DATA_DIR, \"idx_to_chord.json\")\n","\n","# --- Carga dataset + diccionarios + checks rápidos ---\n","MAX_LEN = 112  # canónico\n","def _coerce_to_list(x):\n","    if isinstance(x, list): return x\n","    if isinstance(x, np.ndarray): return x.tolist()\n","    if isinstance(x, str):\n","        try:\n","            v = ast.literal_eval(x)\n","            if isinstance(v, list): return v\n","        except Exception: pass\n","    return []\n","\n","def load_dataset_and_dicts():\n","    if os.path.exists(DATA_PARQUET):\n","        df = pd.read_parquet(DATA_PARQUET)\n","    elif os.path.exists(DATA_CSV):\n","        df = pd.read_csv(DATA_CSV)\n","    else:\n","        raise FileNotFoundError(\"Faltan all_features_transformer.(parquet|csv) en DATA_DIR.\")\n","    for col in [\"encoded_chords\",\"target_chords\",\"attention_mask\"]:\n","        if not isinstance(df[col].iloc[0], (list, np.ndarray)):\n","            df[col] = df[col].apply(_coerce_to_list)\n","    with open(CH2IDX_PATH, \"r\", encoding=\"utf-8\") as f: chord_to_idx = json.load(f)\n","    with open(IDX2CH_PATH, \"r\", encoding=\"utf-8\") as f: idx_to_chord = json.load(f)\n","    pad_idx = chord_to_idx.get(\"[PAD]\", None)\n","    unk_idx = chord_to_idx.get(\"[UNK]\", None)\n","    if pad_idx is None or unk_idx is None or pad_idx != 0:\n","        raise ValueError(\"Vocab inválido: requiere [PAD]=0 y [UNK].\")\n","    # check longitudes y máscara binaria\n","    L = df[\"encoded_chords\"].apply(len)\n","    M = df[\"attention_mask\"].apply(len)\n","    if not (L.eq(MAX_LEN).all() and M.eq(MAX_LEN).all()):\n","        raise ValueError(f\"Longitudes != {MAX_LEN}.\")\n","    mask_vals = set(int(x) for row in df[\"attention_mask\"] for x in row)\n","    if not mask_vals.issubset({0,1}):\n","        raise ValueError(\"attention_mask no binaria.\")\n","    return df, chord_to_idx, idx_to_chord, pad_idx, unk_idx\n","\n","df, chord_to_idx, idx_to_chord, pad_idx, unk_idx = load_dataset_and_dicts()\n","\n","# --- Datasets, splits (80/10/10 estratificado por main_genre), dataloaders ---\n","class ChordsDataset(torch.utils.data.Dataset):\n","    def __init__(self, encoded, target, mask):\n","        self.encoded, self.target, self.mask = encoded, target, mask\n","    def __len__(self): return len(self.encoded)\n","    def __getitem__(self, idx):\n","        return (torch.tensor(self.encoded[idx], dtype=torch.long),\n","                torch.tensor(self.target[idx],  dtype=torch.long),\n","                torch.tensor(self.mask[idx],    dtype=torch.long))\n","\n","def make_splits(df, seed=SEED, test_size=0.1, val_size=0.1):\n","    stratify_col = \"main_genre\" if \"main_genre\" in df.columns else None\n","    df_train, df_temp = train_test_split(df, test_size=(test_size+val_size), random_state=seed,\n","                                         stratify=df[stratify_col] if stratify_col else None)\n","    rel_val = val_size/(test_size+val_size)\n","    df_val, df_test = train_test_split(df_temp, test_size=(1-rel_val), random_state=seed,\n","                                       stratify=df_temp[stratify_col] if stratify_col else None)\n","    return df_train.reset_index(drop=True), df_val.reset_index(drop=True), df_test.reset_index(drop=True)\n","\n","def to_dataset(dframe):\n","    return ChordsDataset(dframe[\"encoded_chords\"].tolist(),\n","                         dframe[\"target_chords\"].tolist(),\n","                         dframe[\"attention_mask\"].tolist())\n","\n","def collate_batch(batch):\n","    enc, tgt, msk = zip(*batch)\n","    return torch.stack(enc), torch.stack(tgt), torch.stack(msk)\n","\n","df_train, df_val, df_test = make_splits(df, seed=SEED, test_size=0.1, val_size=0.1)\n","BATCH_SIZE = 256\n","train_loader = torch.utils.data.DataLoader(to_dataset(df_train), batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_batch)\n","val_loader   = torch.utils.data.DataLoader(to_dataset(df_val),   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n","test_loader  = torch.utils.data.DataLoader(to_dataset(df_test),  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n","\n","# --- Modelo Transformer decoder-only (igual al original) ---\n","@dataclass\n","class ModelConfig:\n","    vocab_size: int; pad_idx: int; unk_idx: int; max_len: int = MAX_LEN\n","    d_model: int = 256; n_layers: int = 4; n_heads: int = 8; d_ff: int = 1024; dropout: float = 0.1\n","\n","class PositionalEmbedding(nn.Module):\n","    def __init__(self, max_len, d_model):\n","        super().__init__(); self.pos_emb = nn.Embedding(max_len, d_model)\n","    def forward(self, x):\n","        B,T = x.size(); pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n","        return self.pos_emb(pos)\n","\n","class CausalTransformer(nn.Module):\n","    def __init__(self, cfg: ModelConfig):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model, padding_idx=cfg.pad_idx)\n","        self.pos_emb = PositionalEmbedding(cfg.max_len, cfg.d_model)\n","        enc = nn.TransformerEncoderLayer(d_model=cfg.d_model, nhead=cfg.n_heads,\n","                                         dim_feedforward=cfg.d_ff, dropout=cfg.dropout,\n","                                         activation=\"gelu\", batch_first=True, norm_first=True)\n","        self.trf = nn.TransformerEncoder(enc, num_layers=cfg.n_layers)\n","        self.drop = nn.Dropout(cfg.dropout)\n","        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size)\n","        self.apply(self._init_w)\n","    def _init_w(self, m):\n","        if isinstance(m, (nn.Linear, nn.Embedding)):\n","            nn.init.normal_(m.weight, 0.0, 0.02)\n","        if isinstance(m, nn.Linear) and m.bias is not None:\n","            nn.init.zeros_(m.bias)\n","    def _causal_mask(self, T, device):\n","        return torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n","    def forward(self, x, attention_mask=None):\n","        B,T = x.shape\n","        h = self.drop(self.tok_emb(x) + self.pos_emb(x))\n","        causal = self._causal_mask(T, x.device)\n","        key_pad = (attention_mask == 0) if attention_mask is not None else None\n","        h = self.trf(h, mask=causal, src_key_padding_mask=key_pad)\n","        return self.lm_head(h)\n","\n","# --- Métricas y utilidades de entrenamiento ---\n","def topk_accuracy(logits, target, k=1, mask=None):\n","    with torch.no_grad():\n","        preds = logits.argmax(-1) if k==1 else logits.topk(k, dim=-1).indices\n","        if mask is None: mask = torch.ones_like(target, dtype=torch.long)\n","        valid = (mask==1) & (target!=pad_idx)\n","        if k==1:\n","            correct = ((preds==target) & valid).sum().item()\n","        else:\n","            tgt_exp = target.unsqueeze(-1).expand_as(preds)\n","            correct = ((preds==tgt_exp) & valid.unsqueeze(-1)).any(dim=-1).sum().item()\n","        total = valid.sum().item()\n","        return correct / max(1, total)\n"]},{"cell_type":"markdown","source":["## ENTRENAMIENTO"],"metadata":{"id":"MxQWgb3pNtGc"}},{"cell_type":"code","source":["# Entrenamiento: misma arquitectura/hiperparámetros, scheduler lineal con warmup y grad clip; guarda checkpoints+config.\n","cfg = ModelConfig(vocab_size=len(chord_to_idx), pad_idx=pad_idx, unk_idx=unk_idx, max_len=MAX_LEN)\n","model = CausalTransformer(cfg).to(device)\n","\n","LR = 2e-4\n","EPOCHS = 15\n","WARMUP_PCT = 0.05\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LR, betas=(0.9,0.999), weight_decay=0.01)\n","\n","def build_scheduler(optimizer, num_steps, warmup_steps):\n","    def lr_lambda(step):\n","        if step < warmup_steps:\n","            return float(step) / float(max(1, warmup_steps))\n","        return max(0.0, float(num_steps - step) / float(max(1, num_steps - warmup_steps)))\n","    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n","\n","num_train_steps = EPOCHS * math.ceil(len(df_train)/BATCH_SIZE)\n","scheduler = build_scheduler(optimizer, num_steps=num_train_steps, warmup_steps=int(num_train_steps*WARMUP_PCT))\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n","\n","history = {\"train_loss\":[], \"val_loss\":[], \"val_top1\":[], \"val_top5\":[]}\n","best_val, best_path = float(\"inf\"), os.path.join(MODELS_DIR, \"checkpoint_best.pt\")\n","\n","def run_epoch(loader, train: bool):\n","    model.train(train)\n","    total_loss, total_items, total_t1, total_t5 = 0.0, 0, 0.0, 0.0\n","    for enc, tgt, msk in loader:\n","        enc, tgt, msk = enc.to(device), tgt.to(device), msk.to(device)\n","        logits = model(enc, attention_mask=msk)\n","        loss = criterion(logits.reshape(-1, cfg.vocab_size), tgt.reshape(-1))\n","        B = enc.size(0)\n","        if train:\n","            optimizer.zero_grad(set_to_none=True)\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step(); scheduler.step()\n","        total_loss += loss.item()*B; total_items += B\n","        total_t1   += topk_accuracy(logits, tgt, k=1, mask=msk)*B\n","        total_t5   += topk_accuracy(logits, tgt, k=5, mask=msk)*B\n","    avg_loss = total_loss / max(1,total_items)\n","    avg_t1   = total_t1   / max(1,total_items)\n","    avg_t5   = total_t5   / max(1,total_items)\n","    return avg_loss, avg_t1, avg_t5\n","\n","for epoch in range(1, EPOCHS+1):\n","    tr_loss, _, _   = run_epoch(train_loader, train=True)\n","    va_loss, v1, v5 = run_epoch(val_loader,   train=False)\n","    history[\"train_loss\"].append(tr_loss); history[\"val_loss\"].append(va_loss)\n","    history[\"val_top1\"].append(v1);        history[\"val_top5\"].append(v5)\n","    # checkpointing\n","    torch.save({\"model_state\": model.state_dict(), \"epoch\": epoch}, os.path.join(MODELS_DIR, \"checkpoint_last.pt\"))\n","    if va_loss < best_val:\n","        best_val = va_loss\n","        torch.save({\"model_state\": model.state_dict(), \"epoch\": epoch}, best_path)\n","\n","# Guardar configuración (para reconstrucción en evaluación/inferencia)\n","with open(os.path.join(MODELS_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n","    json.dump({**asdict(cfg), \"seed\": SEED, \"lr\": LR, \"epochs\": EPOCHS, \"warmup_pct\": WARMUP_PCT}, f)\n","\n","# Evaluación final rápida (test)\n","def evaluate(loader):\n","    model.eval()\n","    total_loss, total_items, total_t1, total_t5 = 0.0, 0, 0.0, 0.0\n","    with torch.no_grad():\n","        for enc, tgt, msk in loader:\n","            enc, tgt, msk = enc.to(device), tgt.to(device), msk.to(device)\n","            logits = model(enc, attention_mask=msk)\n","            loss = criterion(logits.reshape(-1, cfg.vocab_size), tgt.reshape(-1))\n","            B = enc.size(0)\n","            total_loss += loss.item()*B; total_items += B\n","            total_t1 += topk_accuracy(logits, tgt, k=1, mask=msk)*B\n","            total_t5 += topk_accuracy(logits, tgt, k=5, mask=msk)*B\n","    avg_loss = total_loss/max(1,total_items); ppl = math.exp(avg_loss)\n","    return avg_loss, ppl, total_t1/max(1,total_items), total_t5/max(1,total_items)\n","\n","test_loss, test_ppl, test_top1, test_top5 = evaluate(test_loader)\n","print(f\"[TEST] loss={test_loss:.4f} | ppl={test_ppl:.2f} | top1={test_top1:.3f} | top5={test_top5:.3f}\")\n"],"metadata":{"id":"GTqZePh4NtYu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CARGA + EVALUACIÓN"],"metadata":{"id":"YBSsMrV8Nz5t"}},{"cell_type":"code","source":["# Celda 3 — Carga + Evaluación (solo TEST; progreso en CPU; misma lógica que el original)\n","\n","# 1) Cargar dataset y diccionarios\n","df, chord_to_idx, idx_to_chord, pad_idx, unk_idx = load_dataset_and_dicts()\n","\n","# 2) Reconstruir splits (mismo SEED) + diagnóstico\n","df_train, df_val, df_test = make_splits(df, seed=SEED, test_size=0.1, val_size=0.1)\n","print(f\"device={device.type} | val_size={len(df_val)} | test_size={len(df_test)}\")\n","\n","# 3) DataLoaders (como el original: num_workers=0; batch menor en CPU)\n","from torch.utils.data import DataLoader\n","pin = (device.type == \"cuda\")\n","num_workers = 0\n","BATCH_EVAL = 64 if device.type == \"cpu\" else 256\n","val_loader  = DataLoader(to_dataset(df_val),  batch_size=BATCH_EVAL, shuffle=False,\n","                         collate_fn=collate_batch, pin_memory=pin, num_workers=num_workers, persistent_workers=False)\n","test_loader = DataLoader(to_dataset(df_test), batch_size=BATCH_EVAL, shuffle=False,\n","                         collate_fn=collate_batch, pin_memory=pin, num_workers=num_workers, persistent_workers=False)\n","print(f\"BATCH_EVAL={BATCH_EVAL} | pin_memory={pin} | num_workers={num_workers}\")\n","\n","# 4) Reconstruir modelo desde config.json + cargar checkpoint_best.pt\n","import math, time\n","import torch.nn as nn\n","import torch.nn.functional as F\n","with open(os.path.join(MODELS_DIR, \"config.json\"), \"r\", encoding=\"utf-8\") as f:\n","    cfg_json = json.load(f)\n","cfg = ModelConfig(\n","    vocab_size=len(chord_to_idx), pad_idx=pad_idx, unk_idx=unk_idx,\n","    max_len=int(cfg_json.get(\"max_len\", MAX_LEN)),\n","    d_model=int(cfg_json.get(\"d_model\", 256)),\n","    n_layers=int(cfg_json.get(\"n_layers\", 4)),\n","    n_heads=int(cfg_json.get(\"n_heads\", 8)),\n","    d_ff=int(cfg_json.get(\"d_ff\", 1024)),\n","    dropout=float(cfg_json.get(\"dropout\", 0.1))\n",")\n","model = CausalTransformer(cfg).to(device)\n","ckpt = torch.load(os.path.join(MODELS_DIR, \"checkpoint_best.pt\"), map_location=device)\n","model.load_state_dict(ckpt[\"model_state\"])\n","model.eval()\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n","\n","# 5) Evaluación (solo TEST; progreso cada 20 batches; autocast API nueva)\n","if device.type == \"cuda\":\n","    try: torch.set_float32_matmul_precision(\"high\")\n","    except Exception: pass\n","    torch.backends.cudnn.deterministic = False\n","    torch.backends.cudnn.benchmark = True\n","\n","def evaluate(loader, name=\"EVAL\"):\n","    model.eval()\n","    total_loss, total_items = 0.0, 0\n","    total_t1, total_t5 = 0.0, 0.0\n","    use_amp = (device.type == \"cuda\")\n","    total_batches = len(loader)\n","    start = time.time()\n","    with torch.no_grad():\n","        for b, (enc, tgt, msk) in enumerate(loader, 1):\n","            enc = enc.to(device, non_blocking=True)\n","            tgt = tgt.to(device, non_blocking=True)\n","            msk = msk.to(device, non_blocking=True)\n","            with torch.amp.autocast(\"cuda\", enabled=use_amp):\n","                logits = model(enc, attention_mask=msk)\n","                loss = criterion(logits.reshape(-1, cfg.vocab_size), tgt.reshape(-1))\n","            B = enc.size(0)\n","            total_loss += loss.item()*B; total_items += B\n","            total_t1 += topk_accuracy(logits, tgt, k=1, mask=msk)*B\n","            total_t5 += topk_accuracy(logits, tgt, k=5, mask=msk)*B\n","            if b % 20 == 0 or b == total_batches:\n","                print(f\"[{name}] {b}/{total_batches} batches\", end=\"\\r\")\n","    avg_loss = total_loss / max(1, total_items)\n","    ppl = math.exp(avg_loss)\n","    dur = time.time() - start\n","    print(f\"\\n[{name}] done in {dur:.1f}s\")\n","    return avg_loss, ppl, total_t1/max(1,total_items), total_t5/max(1,total_items)\n","\n","# 6) Ejecutar evaluación (solo TEST, como el original)\n","test_loss, test_ppl, test_top1, test_top5 = evaluate(test_loader, name=\"TEST\")\n","print(f\"[TEST] loss={test_loss:.4f} | ppl={test_ppl:.2f} | top1={test_top1:.3f} | top5={test_top5:.3f}\")\n","\n","# 7) Ejemplos cualitativos (top-k en validación; muestra pequeña)\n","def idxs_to_chords(idxs):\n","    return [idx_to_chord.get(str(int(i)), f\"<{int(i)}>\") for i in idxs]\n","\n","def qualitative_examples(df_split, n=6, k=5):\n","    model.eval()\n","    print(\"\\n[Qualitative] Predicción del siguiente acorde (top-k)\")\n","    sample_idx = np.random.choice(len(df_split), size=min(n, len(df_split)), replace=False)\n","    for i in sample_idx:\n","        enc = torch.tensor(df_split[\"encoded_chords\"].iloc[i], dtype=torch.long).unsqueeze(0).to(device, non_blocking=True)\n","        msk = torch.tensor(df_split[\"attention_mask\"].iloc[i], dtype=torch.long).unsqueeze(0).to(device, non_blocking=True)\n","        with torch.inference_mode():\n","            logits = model(enc, attention_mask=msk)\n","            valid_len = int(msk.sum().item()); pos = max(1, valid_len) - 1\n","            next_logits = logits[0, pos, :]\n","            probs = F.softmax(next_logits, dim=-1)\n","            top = torch.topk(probs, k)\n","            pred_idxs = top.indices.tolist()\n","            pred_probs = [float(x) for x in top.values.tolist()]\n","        context = enc[0, max(0, valid_len-8):valid_len].tolist()\n","        print(f\"- idx={i} | contexto últimos 8: {idxs_to_chords(context)}\")\n","        print(f\"  top-{k}: {[(idxs_to_chords([p])[0], pr) for p, pr in zip(pred_idxs, pred_probs)]}\")\n","\n","qualitative_examples(df_val, n=6, k=5)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZOXq2Y1N0PR","executionInfo":{"status":"ok","timestamp":1755104265550,"user_tz":-120,"elapsed":21428,"user":{"displayName":"Carlos Romero","userId":"09804593201902616528"}},"outputId":"ee90d286-adf5-4064-eb8f-256390a28949"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["device=cuda | val_size=30156 | test_size=30157\n","BATCH_EVAL=256 | pin_memory=True | num_workers=0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[TEST] 118/118 batches\n","[TEST] done in 8.2s\n","[TEST] loss=0.8000 | ppl=2.23 | top1=0.764 | top5=0.960\n","\n","[Qualitative] Predicción del siguiente acorde (top-k)\n","- idx=22830 | contexto últimos 8: ['B', 'A#m', 'D#m', 'C#', 'B', 'A#m', 'D#m', 'C#']\n","  top-5: [('B', 0.8965573310852051), ('F#', 0.06570390611886978), ('D#m', 0.026767481118440628), ('G#m', 0.004140423145145178), ('A#m', 0.0032838312909007072)]\n","- idx=8796 | contexto últimos 8: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\n","  top-5: [('A', 0.8251206278800964), ('F#m', 0.1399698406457901), ('D', 0.007280388846993446), ('E7', 0.006934138480573893), ('A7', 0.004347603768110275)]\n","- idx=25843 | contexto últimos 8: ['B7', 'E', 'Bm', 'Am', 'Bm', 'Am', 'B7', 'E']\n","  top-5: [('B7', 0.5017744898796082), ('A', 0.17257916927337646), ('B', 0.11076533794403076), ('F#m', 0.09145978093147278), ('G#7', 0.024557190015912056)]\n","- idx=28339 | contexto últimos 8: ['E', 'A', 'D', 'A', 'E', 'A', 'E', 'A']\n","  top-5: [('D', 0.4962645471096039), ('E', 0.42488986253738403), ('B', 0.017032882198691368), ('F#m', 0.011671301908791065), ('C#m', 0.0058908769860863686)]\n","- idx=1078 | contexto últimos 8: ['A#m', 'C#', 'F#', 'G#', 'A#m', 'C#', 'F#', 'G#']\n","  top-5: [('A#m', 0.9902273416519165), ('F#', 0.005520532373338938), ('C#', 0.0014529061736539006), ('D#m', 0.0008633206016384065), ('D#', 0.0005184969631955028)]\n","- idx=6757 | contexto últimos 8: ['A', 'C', 'D', 'A#', 'C', 'D', 'A#', 'E']\n","  top-5: [('G', 0.512778639793396), ('D', 0.19067615270614624), ('C', 0.07611176371574402), ('A', 0.07011322677135468), ('B', 0.055972836911678314)]\n"]}]}]}