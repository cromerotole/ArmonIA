{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["LFQLnGiNABCE","BLqKDc27CARx"],"authorship_tag":"ABX9TyOZ3I3MYTLwHrUB+WMxNoYr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## SETUP"],"metadata":{"id":"LFQLnGiNABCE"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"8ettNiGo_8tk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755098331915,"user_tz":-120,"elapsed":21790,"user":{"displayName":"Carlos Romero","userId":"09804593201902616528"}},"outputId":"c3ea7cbf-caba-47fa-ffcf-ea00c8345904"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Setup breve: montar Drive, definir rutas y helpers usados por el pipeline.\n","try:\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","except Exception:\n","    pass\n","\n","import os, re, json\n","import pandas as pd\n","import numpy as np\n","\n","# Rutas (I/O)\n","BASE_DIR = \"/content/drive/MyDrive/Colab Notebooks/TFG\"\n","RAW_CSV  = os.path.join(BASE_DIR, \"chordonomicon.csv\")\n","OUT_DIR  = os.path.join(BASE_DIR, \"Archivos preprocesamiento\")\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","# Constantes de preprocesado\n","MAX_LEN = 112  # longitud fija de secuencia para el Transformer\n","\n","# ---- Helpers de limpieza/tokenización ----\n","def remove_section_tags(s: str) -> str:\n","    # Elimina etiquetas <intro_1>, <chorus_2>, etc.\n","    return re.sub(r\"<[^>]+>\", \"\", str(s)).strip()\n","\n","def tokenize_chords(s: str):\n","    # Tokeniza por espacios a lista de acordes.\n","    return str(s).strip().split()\n","\n","def normalize_chord_notation(ch: str) -> str:\n","    # Normalización de notación (idéntica al original).\n","    return ch.replace(\"min\", \"m\").replace(\"maj\", \"M\").replace(\"s\", \"#\")\n","\n","# ---- Vocabulario simplificado + filtro y manejo de slash chords ----\n","ROOTS     = ['C','C#','Db','D','D#','Eb','E','F','F#','Gb','G','G#','Ab','A','A#','Bb','B']\n","QUALITIES = ['', 'm', 'dim', 'aug']\n","EXTENSIONS= ['', '6', '7', 'maj7', 'sus2', 'sus4', 'add9']\n","valid_chords = set(r + q + e for r in ROOTS for q in QUALITIES for e in EXTENSIONS)\n","\n","def filter_valid_or_simplify_chords(chord_list):\n","    # Reduce slash chords a su raíz; filtra solo acordes válidos.\n","    cleaned = []\n","    for chord in chord_list:\n","        if \"/\" in chord:\n","            chord = chord.split(\"/\")[0]\n","        if chord in valid_chords:\n","            cleaned.append(chord)\n","    return cleaned\n","\n","# ---- Objetos de acorde (estructura mínima) ----\n","class ExtendedChord:\n","    # Representa root + quality ('m' si aplica) + extension (resto).\n","    def __init__(self, chord_str: str):\n","        self.original = chord_str\n","        m = re.match(r\"^([A-G][b#]?)(.*)\", chord_str)\n","        if m:\n","            self.root = m.group(1)\n","            remainder = m.group(2)\n","            if remainder.startswith(\"m\"):\n","                self.quality = \"m\"\n","                self.extension = remainder[1:]\n","            else:\n","                self.quality = \"\"\n","                self.extension = remainder\n","        else:\n","            self.root = self.quality = self.extension = None\n","    def __repr__(self):\n","        return f\"{self.root}{self.quality}{self.extension}\"\n","\n","def to_chord_objects(chord_list):\n","    # Convierte lista de strings a objetos válidos ExtendedChord.\n","    out = []\n","    for ch in chord_list:\n","        obj = ExtendedChord(ch)\n","        if obj.root:\n","            out.append(obj)\n","    return out\n","\n","# ---- Tonalidad media y transposición al eje de Do ----\n","NOTES_SHARP    = ['C','C#','D','D#','E','F','F#','G','G#','A','A#','B']\n","NOTE_TO_VALUE  = {n: i for i, n in enumerate(NOTES_SHARP)}\n","NOTE_TO_VALUE.update({'Db':1,'Eb':3,'Gb':6,'Ab':8,'Bb':10})\n","VALUE_TO_NOTE  = {i: n for i, n in enumerate(NOTES_SHARP)}\n","\n","def harmonic_mean_val(chord_objs):\n","    # Media aritmética de raíces (0–11), redondeada mod 12.\n","    vals = [NOTE_TO_VALUE.get(ch.root) for ch in chord_objs if NOTE_TO_VALUE.get(ch.root) is not None]\n","    if not vals:\n","        return None\n","    return int(round(sum(vals) / len(vals))) % 12\n","\n","def transpose_chord_obj(ch: ExtendedChord, interval: int):\n","    # Transpone un acorde 'interval' semitonos hacia abajo.\n","    rv = NOTE_TO_VALUE.get(ch.root)\n","    if rv is None:\n","        return None\n","    new_root = VALUE_TO_NOTE[(rv - interval) % 12]\n","    return f\"{new_root}{ch.quality}{ch.extension}\"\n","\n","def transpose_song(chord_objs, interval: int):\n","    # Transpone toda la canción al eje de Do (si hay media).\n","    if interval is None:\n","        return []\n","    return [c for c in (transpose_chord_obj(ch, interval) for ch in chord_objs) if c is not None]\n","\n","# ---- Vocab y utilidades de codificación ----\n","def build_vocab(normalised_lists):\n","    # Vocab con tokens especiales al inicio.\n","    all_chords = sorted(set(ch for song in normalised_lists for ch in song))\n","    vocab = ['[PAD]', '[UNK]'] + all_chords\n","    chord_to_idx = {ch: i for i, ch in enumerate(vocab)}\n","    idx_to_chord = {i: ch for ch, i in chord_to_idx.items()}\n","    return chord_to_idx, idx_to_chord\n","\n","def encode_and_pad(seq, chord_to_idx, max_len=MAX_LEN):\n","    # Indiza, trunca o paducea a longitud fija.\n","    ids = [chord_to_idx.get(ch, chord_to_idx['[UNK]']) for ch in seq]\n","    return ids + [chord_to_idx['[PAD]']] * (max_len - len(ids)) if len(ids) < max_len else ids[:max_len]\n","\n","def create_target(seq, pad_idx):       # y_t = x_{t+1} + PAD final\n","    return seq[1:] + [pad_idx]\n","\n","def attention_mask_from(seq, pad_idx): # 1=token real, 0=PAD\n","    return [1 if tok != pad_idx else 0 for tok in seq]\n"]},{"cell_type":"markdown","source":["## PREPROCESAMIENTO"],"metadata":{"id":"BLqKDc27CARx"}},{"cell_type":"code","source":["# Pipeline: carga CSV crudo → limpia/normaliza → objetos + tonalidad → transposición a Do → exporta features y artefactos Transformer.\n","\n","# 1) Carga y depuración mínima de filas/columnas\n","df0 = pd.read_csv(RAW_CSV)\n","df = df0.dropna(subset=[\"genres\", \"decade\", \"main_genre\"]).copy()\n","df.drop(columns=[\"release_date\",\"rock_genre\",\"artist_id\",\"spotify_song_id\",\"spotify_artist_id\"],\n","        errors=\"ignore\", inplace=True)\n","\n","# 2) Limpieza + tokenización + normalización + filtro\n","df[\"chords\"] = df[\"chords\"].apply(remove_section_tags).apply(tokenize_chords)\n","df[\"chords\"] = df[\"chords\"].apply(lambda lst: [normalize_chord_notation(ch) for ch in lst])\n","df[\"chords\"] = df[\"chords\"].apply(filter_valid_or_simplify_chords)\n","\n","# 3) A objetos, tonalidad media y transposición al eje de Do\n","df[\"chord_objects\"]    = df[\"chords\"].apply(to_chord_objects)\n","df[\"harmonic_mean\"]    = df[\"chord_objects\"].apply(harmonic_mean_val)\n","df[\"normalised_chords\"] = df.apply(lambda r: transpose_song(r[\"chord_objects\"], r[\"harmonic_mean\"]), axis=1)\n","\n","# 4) Export base: all_features.csv\n","base_cols = [\"id\",\"chords\",\"normalised_chords\",\"genres\",\"main_genre\",\"decade\"]\n","df_base = df[base_cols].copy()\n","df_base.to_csv(os.path.join(OUT_DIR, \"all_features.csv\"), index=False)\n","\n","# 5) Vocab + codificación fija (MAX_LEN=112): JSONs + CSV/Parquet finales\n","chord_to_idx, idx_to_chord = build_vocab(df[\"normalised_chords\"])\n","with open(os.path.join(OUT_DIR, \"chord_to_idx.json\"), \"w\", encoding=\"utf-8\") as f:\n","    json.dump(chord_to_idx, f)\n","with open(os.path.join(OUT_DIR, \"idx_to_chord.json\"), \"w\", encoding=\"utf-8\") as f:\n","    json.dump(idx_to_chord, f)\n","\n","pad_idx = chord_to_idx[\"[PAD]\"]\n","df[\"encoded_chords\"]  = df[\"normalised_chords\"].apply(lambda seq: encode_and_pad(seq, chord_to_idx, MAX_LEN))\n","df[\"target_chords\"]   = df[\"encoded_chords\"].apply(lambda seq: create_target(seq, pad_idx))\n","df[\"attention_mask\"]  = df[\"encoded_chords\"].apply(lambda seq: attention_mask_from(seq, pad_idx))\n","\n","final_cols = [\"id\",\"normalised_chords\",\"encoded_chords\",\"target_chords\",\"attention_mask\",\"genres\",\"main_genre\",\"decade\"]\n","df_final = df[final_cols].copy()\n","df_final.to_csv   (os.path.join(OUT_DIR, \"all_features_transformer.csv\"), index=False)\n","df_final.to_parquet(os.path.join(OUT_DIR, \"all_features_transformer.parquet\"), index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K2p0ZVR4A1CO","executionInfo":{"status":"ok","timestamp":1755098466992,"user_tz":-120,"elapsed":112578,"user":{"displayName":"Carlos Romero","userId":"09804593201902616528"}},"outputId":"28b2bcd8-1008-442d-c849-3f5cbebb1eab"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-607150728.py:4: DtypeWarning: Columns (2,3,5,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df0 = pd.read_csv(RAW_CSV)\n"]}]}]}